# Parkday-Data-Science-Technical-Take-Home
For this project, the objective was described as such:
"Analyze a set of user and meal data and 1) characterize the data for insights, 2) implement meal recommendations, and 3) suggest next steps in a data science pipeline buildout"

With this, the focus of the project was clarified to be fine-grained preference information, meaning information regarding ingredients. 

I analyzed two datasets, one containing user data with liked ingredients and another containing meal data with various data points including ingredients, recipe names, and cuisine type among others. My approach involved data cleaning, exploratory data analysis, and the use of term frequency-inverse document frequency (tf-idf) to calculate the similarity scores between user's liked ingredients and meal ingredients. The resulting meal recommendation system provides users with a list of top 5 recommended recipes based on their liked ingredients.

To begin this task, I first asked some questions to help better understand the wants and needs of a Parkday user. What would I want as a user of this app? What can make Parkday more valuable as a meal recommendation service? What features are critical in making accurate meal recommendations?

By answering these questions, I was able to gain a better understanding of what a good meal recommendation service should aim to accomplish. It should provide personalized recommendations based on preferences and restrictions, it should provide better recommendations over time as it acquires more data on the user, and it should be easy and simple to use.

I started by examining the data to see what I was working with. The user data set contained 186 different user IDs, user names, and their liked ingredients. The meal data contained more information for over 16000 meals-- recipe names, meal IDs, ingredients, flavors with flavor scores, cuisine type, prep times and images for each respective meal. 

The meal dataset required a fair amount of cleaning/manipulation. I first trimmed down the meal IDs such that they only contained their respective integers, and removed the img column as it would not be serving much purpose for this assignment. After attempting to access the values within the ingredients column, I quickly became aware that what appeared to be lists of ingredients were actually strings. I used the 'ast' module to convert these strings to lists such that each individual ingredient of each meal could be accessed for use in the recommendation model. I observed that the same issue was present in the flavor column and acted accordingly, as what appeared as dictionaries for each value were actually strings. I then created five new columns for the meal dataset containing the flavor scores for each flavor of each meal instead of containing them within the dictionary column. Finally, I also added a column identifying the number of ingredients within each meal.

Although approximately one third of the meal dataset contained NaN values for flavor scores, I decided to leave this alone for the time being because the goal of the task would be involving the ingredients, not the flavors.

The user dataset, with less information present, required less data cleaning. However, it had the same issue with user-liked ingredients being strings that looked like lists instead of lists, so I changed these accordingly.

With this, I moved into some exploratory data analysis. I was interested in looking at some of the various qualities of the meal dataset. I found average prep times and average number of ingredients per meal for each cuisine type, and graphed it on a scatterplot that displayed an exponential relationship. I also took a look at the most frequently liked ingredients by users, and saw that surprisingly, parsnip mousseline was liked at a higher frequency than chicken, which were first and second respectively.

If I were to work further on this, I would perform more EDA regarding relationships between user-liked ingredients and meal ingredients to gain some insight into trends between them before setting up a meal recommendation model.

For the recommendation model, I used the sklearn module to implement tf-idf and cosine similarity to give the top 5 meal recommendations for each user. I developed document-term matrices for both the user dataset and meal dataset, established similarity scores, and then produced a dataframe displaying these scores for each meal. Once completed, I formed a dictionary that would store each user's top 5 meals in terms of highest cosine similarity scores. The user names were the keys of this dictionary, so by calling a user's name, their top 5 meals could be accessed.

As it currently stands, this model is fairly primitive, and only accounts for ingredients and not the other data points collected. If I were to take this further in the future, I would try to accomplish a couple of things. First, I would want to be able to allow a user to input a list of ingredients, such that their information becomes appended to the user dataset, and their top 5 meal recommendations are automatically assigned. Additionally, I would like to involve more factors. Further data collection beyond simply liked ingredients could help enhance this model to make even more accurate recommendations. If the model took into account flavor preferences and prep time restrictions, it could provide greater meal suggestions. This data could be collected both directly and indirectly-- the user could submit their time restrictions to help trim down some options, and the model could learn flavor preferences dependent upon what flavors are present in the meals the user has chosen in the past. There is a lot that could be done in terms of expanding upon this, including things such as K-Nearest Neighbors, and possibly employing regression with all of these factors to make meal predictions. 

Regardless, for the purpose of this task and fine-grained preference data, I believe this to be a good start. I learned some new ways of doing things as I worked through some trouble points, gaining experience with the 'ast' module that I hadn't used before, and freshening up on how to employ lambda functions at one point. I also revisited sklearn which I hadn't used since my internship last summer-- learning a little more about tf-idf than I had understood previously. All things considered, I enjoyed working through these datasets and coming up with an early version of a recommendation system for fine-grained preference information. Thank you for the opportunity!

Jake Antmann
